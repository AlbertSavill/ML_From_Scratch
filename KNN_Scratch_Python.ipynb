{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1e9bff62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gonna go through code that doesn't over use libraries (scikit learn) and code the K Nearest Neighbours from scratch\n",
    "# The purpose of this coding practice is to become more knowledgable in Machine Learning\n",
    "# Most of my projects / lessons (I teach believe it or not) relies way too much on libraries, even normalized in my schools\n",
    "# Cirriculum. The problem I have with this is that it kinda dumbs us down as programmers\n",
    "# All we need to do is write a few lines of code, import an algorithm from sckiti learn, fit the data, hyper tune parameters\n",
    "# Then boom, within 20 minutes we suddenly become \"Machine Learning Engineers\"\n",
    "# Yet we don't even know about the mathematic side, the equations, the loss functions, etc...\n",
    "# Of course as a teacher I do teach the theory and math behind these algorithms\n",
    "# But then again I think, what's even the point? Just import everything, edit it a tiny bit then boom, ML algorithm working.\n",
    "# I think it's time to start a new trend in machine learning to try and code everything ourselves to truly get a better understanding\n",
    "# From a technical standpoint, code everything ourselves, be in more control of our Machine Learning algorithms\n",
    "# Yes, I understand how nerdy I am coming accross, especially doing this on a saturday night lol. But getting more technical is fun\n",
    "# \n",
    "#\n",
    "# It's like driving, sure, we can drive an automatic, or we can drive stick and have more control of the car (and then brag about it lol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f88e9604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets get to it, code a K nearest Neighbour algorithm from scratch, I will comment along and document what I learned\n",
    "# How it's different from relying on libraries, and of course rating the difficulty of the process. There's a reason we use libraries after all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1546f293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries (LOL) that we absolutely need\n",
    "from random import randrange\n",
    "from csv import reader\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "24a0d162",
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_column_to_float(dataset, column):\n",
    "    for row in dataset:\n",
    "        row[column] = float(row[column].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0554603f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_column_to_int(dataset, column):\n",
    "    class_values = [row[column] for row in dataset]\n",
    "    unique = set(class_values)\n",
    "    lookup = dict()\n",
    "    for i, value in enumerate(unique):\n",
    "        lookup[value] = i\n",
    "    for row in dataset:\n",
    "        row[column] = lookup[row[column]]\n",
    "    return lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "80bf6aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This searches for the min and max value from each column and row using a for loop in a list in the dataset\n",
    "def dataset_minmax(dataset):\n",
    "    minmax = list()\n",
    "    for i in range(len(dataset[0])):\n",
    "        col_values = [row[i] for row in dataset]\n",
    "        value_min = min(col_values)\n",
    "        value_max = max(col_values)\n",
    "        minmax.append([value_min, value_max])\n",
    "    return minmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1dac0ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizes the dataset with a simple for loop that does subtraction and division with each row in the dataset\n",
    "# Supringsly low amount of code to normalize a dataset, just as easy as using sklearn.preprocessing and numpy\n",
    "# But maybe dont sleep on defining a normalizing function yourself! A simple for loop with somewhat basic math\n",
    "def normalize_dataset(dataset, minmax):\n",
    "    for row in dataset:\n",
    "        for i in range(len(row)):\n",
    "            row[i] = (row[i] - minmax[i][0]) / (minmax[i][1] - minmax[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2e38e836",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data to k fold\n",
    "# Splitting the dataset and creating n folds (k folds) which we will use in our evaluation\n",
    "# Think of this as an alternative to sklearn.model_selection train_test_split()\n",
    "def cross_validation_split(dataset, n_folds):\n",
    "    dataset_split = list()\n",
    "    dataset_copy = list(dataset)\n",
    "    fold_size = int(len(dataset) / n_folds)\n",
    "    for _ in range(n_folds):\n",
    "        fold = list()\n",
    "        while len(fold) < fold_size:\n",
    "            index = randrange(len(dataset_copy))\n",
    "            fold.append(dataset_copy.pop(index))\n",
    "        dataset_split.append(fold)\n",
    "    return dataset_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bc370503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using actual and predicted values to return an accuracy metric\n",
    "def accuracy_metric(actual, predicted):\n",
    "    correct = 0\n",
    "    for i in range(len(actual)):\n",
    "        if actual[i] == predicted[i]:\n",
    "            correct += 1\n",
    "    return correct / float(len(actual)) * 100.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "36508632",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating algorithm, splitting the folds in train and test sets\n",
    "# Once dataset is split, we use the predicted and actual set within an accuracy metric\n",
    "def evaluate_algorithm(dataset, algorithm, n_folds, *args):\n",
    "    folds = cross_validation_split(dataset, n_folds)\n",
    "    scores = list()\n",
    "    for fold in folds:\n",
    "        train_set = list(folds)\n",
    "        train_set.remove(fold)\n",
    "        train_Set = sum(train_set, [])\n",
    "        test_set = list()\n",
    "        for row in fold:\n",
    "            row_copy = list(row)\n",
    "            test_set.append(row_copy)\n",
    "            row_copy[-1] = None\n",
    "        predicted = algorithm(train_set, test_set, *args)\n",
    "        actual = [row[-1] for row in fold]\n",
    "        accuracy = accuracy_metric(actual, predicted)\n",
    "        scores.append(accuracy)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0c1c8b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Euclidean distance between 2 vectors\n",
    "# Formula: d = âˆš[ (x2 - x1)^2 + (y2 - y1)^2]\n",
    "def euclidean_distance(row1, row2):     # creating seperate rows for x and y where x1, y1 are cooridnates while x2, y2 are coordinates for a different point\n",
    "    distance = 0.0\n",
    "    for i in range(len(row1)-1):\n",
    "        distance += (row1[i] - row2[i])**2 # subtracting y and x while also adding them up outside the bracket\n",
    "    return sqrt(distance) # square d (distance between points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "86d2e640",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Locating similar neighbours via their distance\n",
    "# Calculatig the distance between x and y in the train and test data\n",
    "# Creating neighbours based on the distances calculated using euclidean distance\n",
    "def get_neighbours(train, test_row, num_neighbours):\n",
    "    distances = list()\n",
    "    for train_row in train:\n",
    "        dist = euclidean_distance(test_row, train_row)\n",
    "        distances.append((train_row, dist))\n",
    "    distances.sort(key = lambda tup: tup[1])\n",
    "    neighbours = list()\n",
    "    for i in range(num_neighbours):\n",
    "        neighbours.append(distances[i][0])\n",
    "    return neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ef2a80da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicitng classification with neighbours\n",
    "# Returning the outputs from the neighbours object as our prediction\n",
    "def predict_classification(train, test_row, num_neighbours):\n",
    "    neighbours = get_neighbours(train, test_row, num_neighbours)\n",
    "    output_values = [row[-1] for row in neighbours]\n",
    "    prediction = max(set(output_values), key = output_values.count)\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "48a7246d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally the KNN Algorithm\n",
    "# Define the KNN with train, test, and number of neighbours as our paramters\n",
    "# Using those paramters to predict the classification\n",
    "# appending the output into the prediction list which should be printed in our results.\n",
    "def KNN(train, test, num_neighbours):\n",
    "    predictions = list()\n",
    "    for row in test:\n",
    "        output = predict_classification(train, row, num_neighbours)\n",
    "        predictions.append(output)\n",
    "    return(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9594cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Okay, instantly way more code compared to using a bunch of libraries\n",
    "# A lot more math too. This should be expected in Machine Learning\n",
    "# But forcing myself to create a bunch of functions to combine together at the end gave me a much better understanding\n",
    "# Of how the KNN works and more importantly the formula of the euclidean distance.\n",
    "# If I was to do another KNN project with the usual sklearn functions I would have never coded out the euclidean distance myself\n",
    "# Nor normalize the data myself, nor split the data myself, nor code everything else myself etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70062dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is a good idea do try and get into a habit of coding like this (especially if you want to become a machine learning engineer)\n",
    "# BUT, coding like this is more time consuming, and has a lot more lines of code which can increase the chance of errors and debugging\n",
    "# Which results in lost time compared to someone who uses the appropriate libraries.\n",
    "# But once again it never hurts to get a better understanding of the math, and doing it in a practical way helps a lot more rather\n",
    "# Than reading the formula on google and pretending to know what it means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239e46a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This was honestly a little challenging and quite fun.\n",
    "# I would like to do this more and even dip my feet into deep learning without the use of Tensorflow / Keras\n",
    "# However, I would rather do deep learning in c++ as I am sick of doing it in python where I have to wait 12 hours for the epochs\n",
    "# I'm still learning c++ so give me some time before I do the deep learning type of \"from scratch\" projects."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
